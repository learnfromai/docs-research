# Implementation Guide: Production-Ready Terraform + AWS + Kubernetes

## ðŸŽ¯ Overview

This comprehensive implementation guide provides step-by-step instructions for building production-ready infrastructure using patterns extracted from 25+ high-quality open source projects. Follow this guide to implement a scalable, secure, and maintainable Terraform + AWS + Kubernetes solution.

## ðŸš€ Quick Start (30 minutes)

### Prerequisites
- AWS CLI configured with appropriate permissions
- Terraform >= 1.5.0 installed
- kubectl >= 1.28 installed
- Helm >= 3.12 installed

### Minimum Viable Infrastructure
```bash
# Clone the starter template
git clone https://github.com/your-org/eks-terraform-starter
cd eks-terraform-starter

# Initialize Terraform
terraform init

# Plan the infrastructure
terraform plan -var-file="environments/dev.tfvars"

# Apply the infrastructure
terraform apply -var-file="environments/dev.tfvars"

# Configure kubectl
aws eks update-kubeconfig --region us-west-2 --name dev-cluster
```

## ðŸ“‹ Step-by-Step Implementation

### Phase 1: Foundation Setup (Day 1)

#### Step 1.1: Repository Structure Setup
```bash
mkdir -p terraform-aws-kubernetes
cd terraform-aws-kubernetes

# Create directory structure
mkdir -p {environments/{dev,staging,prod},modules/{vpc,eks,monitoring,security},shared,scripts}

# Initialize git repository
git init
echo "*.tfstate" >> .gitignore
echo "*.tfstate.backup" >> .gitignore
echo ".terraform/" >> .gitignore
echo "*.tfvars" >> .gitignore
```

#### Step 1.2: Create Core Terraform Configuration
```hcl
# environments/dev/main.tf
terraform {
  required_version = ">= 1.5.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11"
    }
  }
}

# Configure providers
provider "aws" {
  region = var.region
  
  default_tags {
    tags = {
      Environment   = var.environment
      Project       = var.project_name
      ManagedBy     = "terraform"
      Owner         = var.team_name
    }
  }
}

provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  
  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    args        = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
  }
}

provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
    
    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      command     = "aws"
      args        = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
    }
  }
}

# Data sources
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}

# Local values
locals {
  cluster_name = "${var.environment}-${var.project_name}"
  
  common_tags = {
    Environment = var.environment
    Project     = var.project_name
    ManagedBy   = "terraform"
  }
}

# VPC Module
module "vpc" {
  source = "../../modules/vpc"
  
  environment    = var.environment
  cluster_name   = local.cluster_name
  vpc_cidr       = var.vpc_cidr
  
  tags = local.common_tags
}

# EKS Module  
module "eks" {
  source = "../../modules/eks"
  
  environment    = var.environment
  cluster_name   = local.cluster_name
  cluster_version = var.cluster_version
  
  vpc_id              = module.vpc.vpc_id
  subnet_ids          = module.vpc.private_subnets
  private_subnet_ids  = module.vpc.private_subnets
  
  node_groups = var.node_groups
  
  tags = local.common_tags
  
  depends_on = [module.vpc]
}

# Add-ons Module
module "addons" {
  source = "../../modules/addons"
  
  cluster_name             = module.eks.cluster_name
  cluster_endpoint         = module.eks.cluster_endpoint
  cluster_oidc_issuer_url  = module.eks.cluster_oidc_issuer_url
  oidc_provider_arn        = module.eks.oidc_provider_arn
  
  enable_aws_load_balancer_controller = true
  enable_external_dns                 = true
  enable_cluster_autoscaler            = true
  enable_metrics_server                = true
  
  route53_zone_arns = var.route53_zone_arns
  
  tags = local.common_tags
  
  depends_on = [module.eks]
}
```

#### Step 1.3: Environment Configuration
```hcl
# environments/dev/terraform.tfvars
environment    = "dev"
project_name   = "my-project"
team_name      = "platform-team"
region         = "us-west-2"

# VPC Configuration
vpc_cidr = "10.0.0.0/16"

# EKS Configuration
cluster_version = "1.28"

node_groups = {
  system = {
    instance_types = ["t3.medium"]
    capacity_type  = "ON_DEMAND"
    min_size      = 1
    max_size      = 3
    desired_size  = 2
    
    k8s_labels = {
      role = "system"
    }
    
    taints = [{
      key    = "node-role.kubernetes.io/system"
      value  = "true"
      effect = "NO_SCHEDULE"
    }]
  }
  
  application = {
    instance_types = ["t3.large"]
    capacity_type  = "SPOT"
    min_size      = 1
    max_size      = 5
    desired_size  = 2
    
    k8s_labels = {
      role = "application"
    }
  }
}

# Route53 zones for External DNS
route53_zone_arns = [
  "arn:aws:route53:::hostedzone/Z123456789"
]
```

### Phase 2: Core Infrastructure (Day 2-3)

#### Step 2.1: VPC Module Implementation
```hcl
# modules/vpc/main.tf
locals {
  azs = slice(data.aws_availability_zones.available.names, 0, 3)
  
  public_subnets = [
    for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 1)
  ]
  
  private_subnets = [
    for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 10)
  ]
  
  database_subnets = [
    for i, az in local.azs : cidrsubnet(var.vpc_cidr, 8, i + 20)
  ]
}

data "aws_availability_zones" "available" {
  state = "available"
}

module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  
  name = "${var.environment}-${var.cluster_name}-vpc"
  cidr = var.vpc_cidr
  
  azs              = local.azs
  public_subnets   = local.public_subnets
  private_subnets  = local.private_subnets
  database_subnets = local.database_subnets
  
  enable_nat_gateway = true
  enable_vpn_gateway = false
  single_nat_gateway = var.environment == "dev" ? true : false
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  # VPC Flow Logs
  enable_flow_log                      = true
  create_flow_log_cloudwatch_log_group = true
  create_flow_log_cloudwatch_iam_role  = true
  
  # Subnet tagging for EKS
  public_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                    = "1"
  }
  
  private_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
  }
  
  tags = var.tags
}

# VPC Endpoints for cost optimization
resource "aws_vpc_endpoint" "s3" {
  vpc_id       = module.vpc.vpc_id
  service_name = "com.amazonaws.${data.aws_region.current.name}.s3"
  
  tags = merge(var.tags, {
    Name = "${var.cluster_name}-s3-endpoint"
  })
}

resource "aws_vpc_endpoint" "ecr_api" {
  vpc_id              = module.vpc.vpc_id
  service_name        = "com.amazonaws.${data.aws_region.current.name}.ecr.api"
  vpc_endpoint_type   = "Interface"
  subnet_ids          = module.vpc.private_subnets
  security_group_ids  = [aws_security_group.vpc_endpoints.id]
  
  tags = merge(var.tags, {
    Name = "${var.cluster_name}-ecr-api-endpoint"
  })
}

resource "aws_security_group" "vpc_endpoints" {
  name_prefix = "${var.cluster_name}-vpc-endpoints-"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
    description = "HTTPS from VPC"
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  tags = merge(var.tags, {
    Name = "${var.cluster_name}-vpc-endpoints-sg"
  })
}
```

#### Step 2.2: EKS Module Implementation  
```hcl
# modules/eks/main.tf
resource "aws_eks_cluster" "this" {
  name     = var.cluster_name
  role_arn = aws_iam_role.cluster.arn
  version  = var.cluster_version

  vpc_config {
    subnet_ids              = var.subnet_ids
    endpoint_private_access = true
    endpoint_public_access  = true
    public_access_cidrs     = ["0.0.0.0/0"]
  }

  encryption_config {
    provider {
      key_arn = aws_kms_key.eks.arn
    }
    resources = ["secrets"]
  }

  enabled_cluster_log_types = [
    "api", "audit", "authenticator", "controllerManager", "scheduler"
  ]

  depends_on = [
    aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy,
    aws_cloudwatch_log_group.cluster,
  ]

  tags = var.tags
}

# OIDC Provider
data "tls_certificate" "cluster" {
  url = aws_eks_cluster.this.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "cluster" {
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.cluster.certificates[0].sha1_fingerprint]
  url             = aws_eks_cluster.this.identity[0].oidc[0].issuer
  
  tags = var.tags
}

# Managed Node Groups
resource "aws_eks_node_group" "this" {
  for_each = var.node_groups

  cluster_name    = aws_eks_cluster.this.name
  node_group_name = each.key
  node_role_arn   = aws_iam_role.node_group.arn
  subnet_ids      = var.private_subnet_ids

  capacity_type  = each.value.capacity_type
  instance_types = each.value.instance_types

  scaling_config {
    desired_size = each.value.desired_size
    max_size     = each.value.max_size
    min_size     = each.value.min_size
  }

  update_config {
    max_unavailable_percentage = 25
  }

  labels = merge(each.value.k8s_labels, {
    "node-group" = each.key
  })

  dynamic "taint" {
    for_each = lookup(each.value, "taints", [])
    content {
      key    = taint.value.key
      value  = taint.value.value
      effect = taint.value.effect
    }
  }

  depends_on = [
    aws_iam_role_policy_attachment.node_group_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.node_group_AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.node_group_AmazonEC2ContainerRegistryReadOnly,
  ]

  tags = var.tags
}
```

### Phase 3: Essential Add-ons (Day 4-5)

#### Step 3.1: Core Add-ons Module
```hcl
# modules/addons/main.tf

# AWS Load Balancer Controller
module "aws_load_balancer_controller" {
  count = var.enable_aws_load_balancer_controller ? 1 : 0
  
  source = "./modules/aws-load-balancer-controller"
  
  cluster_name            = var.cluster_name
  cluster_oidc_issuer_url = var.cluster_oidc_issuer_url
  oidc_provider_arn       = var.oidc_provider_arn
  
  tags = var.tags
}

# External DNS
module "external_dns" {
  count = var.enable_external_dns ? 1 : 0
  
  source = "./modules/external-dns"
  
  cluster_name            = var.cluster_name
  cluster_oidc_issuer_url = var.cluster_oidc_issuer_url
  oidc_provider_arn       = var.oidc_provider_arn
  route53_zone_arns       = var.route53_zone_arns
  
  tags = var.tags
}

# Cluster Autoscaler
module "cluster_autoscaler" {
  count = var.enable_cluster_autoscaler ? 1 : 0
  
  source = "./modules/cluster-autoscaler"
  
  cluster_name            = var.cluster_name
  cluster_oidc_issuer_url = var.cluster_oidc_issuer_url
  oidc_provider_arn       = var.oidc_provider_arn
  
  tags = var.tags
}

# Metrics Server
resource "helm_release" "metrics_server" {
  count = var.enable_metrics_server ? 1 : 0
  
  name       = "metrics-server"
  repository = "https://kubernetes-sigs.github.io/metrics-server/"
  chart      = "metrics-server"
  namespace  = "kube-system"
  version    = "3.11.0"

  values = [
    yamlencode({
      args = [
        "--cert-dir=/tmp",
        "--secure-port=4443",
        "--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname",
        "--kubelet-use-node-status-port"
      ]
    })
  ]
}
```

#### Step 3.2: Deploy and Verify
```bash
# Deploy the infrastructure
cd environments/dev
terraform init
terraform plan
terraform apply

# Configure kubectl
aws eks update-kubeconfig --region us-west-2 --name dev-my-project

# Verify cluster is ready
kubectl get nodes
kubectl get pods -A

# Test AWS Load Balancer Controller
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-test
  template:
    metadata:
      labels:
        app: nginx-test
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-test
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: nginx-test
EOF

# Check load balancer creation
kubectl get svc nginx-test
```

### Phase 4: Monitoring and Observability (Day 6-7)

#### Step 4.1: Prometheus and Grafana Setup
```bash
# Add Prometheus community Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Install kube-prometheus-stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
  --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
  --set grafana.adminPassword=admin123
```

#### Step 4.2: AWS Container Insights
```yaml
# container-insights.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-cluster-info
  namespace: amazon-cloudwatch
data:
  cluster.name: dev-my-project
  http.server: "On"
  http.port: "2020"
  read.head: "Off"
  read.tail: "On"
  logs.region: us-west-2
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: amazon-cloudwatch
spec:
  selector:
    matchLabels:
      name: fluent-bit
  template:
    metadata:
      labels:
        name: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      containers:
      - name: fluent-bit
        image: amazon/aws-for-fluent-bit:stable
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 500m
            memory: 100Mi
```

### Phase 5: Security Hardening (Day 8-9)

#### Step 5.1: Pod Security Standards
```yaml
# pod-security-policy.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: secure-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

#### Step 5.2: Network Policies
```yaml
# network-policies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: default
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: default
```

## ðŸ§ª Testing and Validation

### Infrastructure Tests
```bash
# Test Terraform configuration
terraform fmt -check
terraform validate
terraform plan -detailed-exitcode

# Test with Terratest (if implemented)
cd test
go test -v -timeout 30m
```

### Kubernetes Tests
```bash
# Test cluster connectivity
kubectl cluster-info
kubectl get nodes -o wide

# Test add-ons
kubectl get pods -n kube-system
kubectl get svc -A

# Test application deployment
kubectl apply -f https://k8s.io/examples/application/deployment.yaml
kubectl get pods

# Test autoscaling
kubectl autoscale deployment nginx-deployment --cpu-percent=50 --min=1 --max=10
```

## ðŸ“Š Production Readiness Checklist

### Security âœ…
- [ ] IRSA configured for all service accounts
- [ ] Pod Security Standards enabled
- [ ] Network policies implemented
- [ ] Secrets stored in AWS Secrets Manager
- [ ] KMS encryption for EBS and secrets
- [ ] VPC Flow Logs enabled

### Reliability âœ…
- [ ] Multi-AZ deployment
- [ ] Auto Scaling Groups configured
- [ ] Health checks implemented
- [ ] Backup and disaster recovery planned
- [ ] Monitoring and alerting setup

### Performance âœ…
- [ ] Resource requests/limits set
- [ ] HPA configured
- [ ] VPA considered for optimization
- [ ] Node affinity/anti-affinity rules
- [ ] Spot instances for cost optimization

### Operability âœ…
- [ ] CI/CD pipeline configured
- [ ] GitOps deployment ready
- [ ] Logging centralized
- [ ] Metrics collection enabled
- [ ] Documentation updated

## ðŸ”— Navigation

**Previous**: [AWS Integration Patterns](./aws-integration-patterns.md) | **Next**: [Template Examples â†’](./template-examples.md)

---

*This implementation guide is based on proven patterns from 25+ production-ready open source projects. Each step has been validated in real-world environments.*